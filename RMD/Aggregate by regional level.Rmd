---
title: "Aggregate by admin1"
output: html_document
date: "2024-04-11"
---

# Aggregate by admin1, i.e. regional/department level to merge with other data

## Import the GRD Data

```{r}
library(tidyr)
library(tidyverse)
library(here)
library(haven)

# Import the data
grd <- read_dta(here("Data", "GRD Dataset", "dfhsw_GRD_public_v1.dta"))

grd |> 
  head()

```


## Filter for country and data range
Now let's filter for Colombia and the year range matching the conflict dataset. We also drop some irrelevant columns. I include 1994 to form the baseline lootability estimate (before sample period starting 1995). 1994 is the earliest year in the dataset for Colombia.

```{r}
# Filter for Colombia
colombia_grd <- grd |> 
  filter(country == "colombia") |> 
  filter(year >= 1994 & year <= 2014 & year != 2000) |> 
  select(-c("COWcode", "wb_ccode", "gwno", "log_comtrade_val", "log_wb_val", "log_usgs_val", "log_multicolour_val"))

colombia_grd |>
  head()
```
## Clean admin1 data
```{r}
# First display how many unique values there are of admin1
unique_admin1 <- colombia_grd |> 
  select(admin1) |> 
  distinct() |> 
  nrow()

print(paste("Unique admin1 values:", unique_admin1))

# Print the names of admin1 unique values
unique_admin1_names <- colombia_grd |> 
  select(admin1) |> 
  distinct() |> 
  pull()

print(unique_admin1_names)
```

We should expect 27 rows in the final dataset (maybe according to total number of departments but we only have 23 here.

* There is a spelling mistake "atioquia" which should be "antioquia". 
* "guajira" is the same as "la guajira" so I will rename them. "barrancabermeja locale" is actually a city in Santander department, so I will rename those observations. 

* "chusaca" is actually part of the "cundinamarca" department.
* "" is also strange. Based on the coordinates of this observation, i.e. 21907, I put it in the "bolivar" department

We will correct this in the next step.

```{r}
# Correct the spelling mistake
colombia_grd <- colombia_grd %>%
  mutate(admin1 = ifelse(admin1 == "atioquia", "antioquia", admin1)) |> 
  mutate(admin1 = ifelse(admin1 == "guajira", "la guajira", admin1)) |>
  mutate(admin1 = ifelse(admin1 == "barrancabermeja locale", "santander", admin1)) |>
  mutate(admin1 = ifelse(admin1 == "chusaca", "cundinamarca", admin1)) |>
  mutate(admin1 = ifelse(obs_no == 21907, "bolivar", admin1)) |> 
  mutate(admin1 = ifelse(admin1 == "la guajra", "la guajira", admin1))
```


So we should have 27 rows at the end of this process.

Let's check the admin1 names again to make sure.

```{r}
# Display the unique admin1 values after correction
colombia_grd |> 
  select(admin1) |> 
  distinct() |> 
  nrow()

# SHow names of admin1 values
colombia_grd |> 
  select(admin1) |> 
  distinct() |> 
  pull()

```


## Remove duplicates from GRD Colombia Dataset

Let's check if there are any duplicates, i.e. all variable entries are the same except obs_no.

```{r}
# Check for duplicates
colombia_grd |>
  select(-obs_no) |>
  duplicated() |>
  sum()

```
There are 21 duplicates. Let's remove them.

```{r}
# Step 1: Add an identifier for each group of duplicates (except for 'obs_no')
resources_marked <- colombia_grd %>%
  group_by(across(-obs_no)) %>% 
  mutate(dup_id = row_number()) %>%
  ungroup()

# Step 2: Keep only the first of each set of duplicates
colombia_filtered <- resources_marked %>%
  filter(dup_id == 1) %>%
  select(-dup_id)  # Removing the temporary 'dup_id' column

# Optional: Check the new size of the dataset
print(paste("New dataset size:", nrow(colombia_filtered)))
```

## Anticipate Rows


# Aggregate admin1-year pairs to one row (same resource type)

```{r}
# Count the number of PRIO_GID-year pairs for each resource type
duplicates_count <- colombia_filtered %>%
  group_by(admin1, year, resource) %>%
  summarise(count = n(), .groups = 'drop') %>%
  filter(count > 1)

# Display the counts of duplicates
print(duplicates_count)

# Optionally, to see the total number of duplicates
total_duplicates <- sum(duplicates_count$count) - nrow(duplicates_count)
print(paste("Total number of duplicate observations:", total_duplicates))
```
So we should expect to remove 301 observations after aggregating by same resource.

```{r}
# Define lists of columns based on their aggregation rule
sum_columns <- c("annuallocationcapacity", "comtrade_value", "wb_value", "usgs_value", "multicolour_value",
                 "world_val_nomc", "world_val_withmc", "wd_annual_value_location1", "wd_annual_value_location2",
                 "exp_annual_value_location1", "exp_annual_value_location2")

# Define a custom summarise function for non-sum columns
concat_if_different <- function(x) {
  unique_x <- unique(x)
  if(length(unique_x) == 1) {
    return(as.character(unique_x))
  } else {
    return(paste(unique_x, collapse = "; "))
  }
}

colombia_aggregated <- colombia_filtered %>%
  group_by(admin1, year, resource) %>%
  summarise(across(all_of(sum_columns), sum, na.rm = TRUE), # Sum the defined columns
            across(-all_of(sum_columns), concat_if_different), # Concatenate if different for the rest
            .groups = 'drop') # Prevents the result from being grouped

```

## Verify same-resource aggregation
Let's check if the aggregation was successful.

```{r}
# Check if the aggregation was successful
duplicates_after_aggregation <- colombia_aggregated %>%
  group_by(admin1, year, resource) %>%
  summarise(count = n(), .groups = 'drop') %>%
  filter(count > 1)

# Check if there are any duplicates left
if (nrow(duplicates_after_aggregation) > 0) {
  print("There are remaining duplicates after aggregation.")
} else {
  print("No duplicates remain after aggregation. Aggregation was successful.")
}

```
# Aggregate admin1-year pairs to one row (different resource types)
Now we aggregate the rows with the same admin1 and year but different resources. We will keep the resource-invariant variables as they are and create new variables for the resource-variant variables by reshaping the data.


Now let's aggregate the data. 
```{r}
columns_to_reshape <- setdiff(names(colombia_aggregated), c("admin1", "year"))

# Pivot the dataset to wide format
colombia_wide <- colombia_aggregated %>%
  pivot_wider(names_from = resource, names_prefix = "res_", 
              values_from = all_of(columns_to_reshape),
              values_fill = list(value = NA)) # Fill missing values with NA

# Note: The values_fill argument is set to fill with NA for simplicity, 
# adjust as needed based on your specific requirements

# Print the result to check
print(colombia_wide)

# Count the final number of columns to verify
final_num_columns <- ncol(colombia_wide)
print(paste("Final number of columns:", final_num_columns))
```

```{r}
# --- Step 1: Verify Initial Conditions Before Reshaping ---
# Count unique admin1-year combinations in the aggregated data
unique_admin1_year_combinations <- nrow(distinct(colombia_aggregated, admin1, year))

# Calculate the expected number of columns after reshaping
num_unique_resources <- length(unique(colombia_aggregated$resource))
columns_to_reshape <- setdiff(names(colombia_aggregated), c("admin1", "year"))
expected_columns_after_reshaping <- 2 + (num_unique_resources * length(columns_to_reshape))

# --- Reshaping (already provided) ---
colombia_wide <- colombia_aggregated %>%
  pivot_wider(names_from = resource, names_prefix = "res_", 
              values_from = all_of(columns_to_reshape),
              values_fill = list(value = NA))

# --- Step 3: Verification after Reshaping ---
# Verify number of rows matches unique admin1-year combinations
actual_rows_reshaped <- nrow(colombia_wide)
if (actual_rows_reshaped == unique_admin1_year_combinations) {
  print("Rows verification passed: Each observation has a unique admin1-year pair.")
} else {
  print("Rows verification failed: Mismatch in expected and actual number of unique admin1-year pairs.")
}

# Verify the number of columns
actual_columns_reshaped <- ncol(colombia_wide)
if (actual_columns_reshaped == expected_columns_after_reshaping) {
  print("Columns verification passed: The number of columns matches the expected number after reshaping.")
} else {
  print(paste("Columns verification failed: Expected", expected_columns_after_reshaping, "columns, but found", actual_columns_reshaped, "columns."))
}

# Check for uniqueness of admin1-year pairs in the reshaped dataset
unique_pairs_post_reshape <- colombia_wide %>%
  distinct(admin1, year) %>%
  nrow()
if (unique_pairs_post_reshape == actual_rows_reshaped) {
  print("Post-reshaping verification passed: Each admin1-year pair is unique in the reshaped dataset.")
} else {
  print("Post-reshaping verification failed: Some admin1-year pairs may not be unique.")
}

```
## Create aggregate lootability column
### Naive lootability (standardise units and sum volume)

Let's standardise the volume units and sum the volumes across resources to create a lootability measure. The standardisation information is in each of the "standardmeasure" columns

```{r}
# Standardising the volume units

# Show columns containing the string "standardmeasure"
standard_measure_columns <- names(colombia_wide)[grepl("standardmeasure", names(colombia_wide))]
print(standard_measure_columns)

# Extract data from these standard measure columns
standard_measure_data <- colombia_wide[standard_measure_columns]

# Gather all the data into one vector to find unique values
all_standard_measure_values <- unlist(standard_measure_data, use.names = FALSE)

# Find unique values across all standard measure columns
distinct_standard_measure_values <- unique(all_standard_measure_values)

# Print the distinct values found in standard measure columns
print("Distinct Values in Standard Measure Columns:")
print(distinct_standard_measure_values)

```
Let's investigate the NA standard measure column

```{r}
# Find the column with NA values in standard measure columns
na_standard_measure_columns <- standard_measure_columns[sapply(colombia_wide[standard_measure_columns], function(x) any(is.na(x)))]

# Print the columns with NA values
print("Columns with NA values in Standard Measure Columns:")
print(na_standard_measure_columns)

```


```{r}
# Step 1: Identify all columns that are likely to contain density values
density_columns <- names(colombia_wide)[grepl("^density_res", names(colombia_wide))]

# Step 2: Calculate the number of NA values in each density column
na_counts_in_density_columns <- sapply(colombia_wide[density_columns], function(x) sum(is.na(x)))

# Print the results
print("NA Counts in Density Columns:")
print(na_counts_in_density_columns)

print(density_columns)

```

Create standardised aggregate lootability measure by summing up the values of lootable resources produced in admin1 in each year.

```{r}
library(dplyr)

# Find all lootable flag columns
lootable_flags <- names(colombia_wide)[grepl("^lootable_", names(colombia_wide))]

# Assuming lootable_flags are defined, as well as the corresponding world_value_columns
world_value_columns <- gsub("lootable_", "wd_annual_value_location1_", lootable_flags)

# Convert the data frame to a tibble for better handling in dplyr
colombia_wide <- as_tibble(colombia_wide)

# Create a new dataframe to store the results
aggregate_lootability <- tibble(admin1 = character(), year = integer(), total_lootable_value = numeric())

# Loop through each lootable resource and perform aggregation
for (i in seq_along(lootable_flags)) {
    lootable_col <- lootable_flags[i]
    value_col <- world_value_columns[i]

    # Check if the current value column exists in the dataframe
    if (value_col %in% names(colombia_wide)) {
        # Filter and aggregate using dplyr
        temp_agg <- colombia_wide %>%
            filter(!!sym(lootable_col) == 1) %>%  # Filter for lootable resources
            group_by(admin1, year) %>%
            summarise(
                lootable_value = sum(!!sym(value_col), na.rm = TRUE),
                .groups = 'drop'
            ) %>%
            rename(total_lootable_value = lootable_value)  # Rename to a common column for easier merging

        # Merge or bind rows with the main aggregate dataframe
        if (nrow(aggregate_lootability) == 0) {
            aggregate_lootability <- temp_agg
        } else {
            # If already initialized, merge the new data
            aggregate_lootability <- full_join(aggregate_lootability, temp_agg, by = c("admin1", "year")) %>%
                rowwise() %>%
                mutate(total_lootable_value = sum(c_across(starts_with("total_lootable_value")), na.rm = TRUE)) %>%
                select(admin1, year, total_lootable_value)
        }
    }
}

# Join the aggregated lootability data back to the main dataframe
colombia_wide <- left_join(colombia_wide, aggregate_lootability, by = c("admin1", "year"))

# Replace NA values with zero in the lootability column
colombia_wide$total_lootable_value <- replace_na(colombia_wide$total_lootable_value, 0)

# Check the results
print(head(colombia_wide))


```

Now, normalise lootability measure by dividing by total value of production of all resources in that year. Calculate lootability measure with world_value_location1 columns

```{r}
colombia_wide <- colombia_wide %>%
  # Convert to tibble if not already
  as_tibble() %>%
  # Calculate the total value of all world value columns
  mutate(total_value = rowSums(select(., all_of(world_value_columns)), na.rm = TRUE)) %>%
  # Use the previously created total_lootable_value to calculate lootability proportion
  mutate(lootability_proportion = case_when(
    total_value > 0 ~ total_lootable_value / total_value,
    TRUE ~ 0  # Avoid division by zero by setting proportion to 0 when total_value is 0
  ))

# Print head
print(head(colombia_wide))
```

## Check consistency of resource-invariant variables

```{r}
library(dplyr)
library(tidyr)

# Define resource-invariant variables
resource_invariant_vars <- c("country", "region_wb",
                             "continent", "admin1", "admin2", "gid_centroid_latitude", "gid_centroid_longitude")

# Function to check for consistent values across all reshaped columns of a given invariant variable
check_invariant_consistency <- function(data, invariant_var) {
  # Generate the pattern to match the reshaped column names for the invariant variable
  pattern <- paste0("^", invariant_var, "_res_")
  
  # Identify all columns for the invariant variable
  columns <- grep(pattern, names(data), value = TRUE)
  
  # Iterate over each row to check consistency of the variable's values
  consistent_rows <- apply(data[columns], 1, function(row) {
    unique_values <- unique(na.omit(row))
    length(unique_values) == 1
  })
  
  # Return the proportion of rows that are consistent
  mean(consistent_rows, na.rm = TRUE)
}

# Check consistency for each resource-invariant variable and collect results
consistency_results <- sapply(resource_invariant_vars, function(var) {
  check_invariant_consistency(colombia_wide, var)
})

# Print the consistency check results
print("Consistency check results for resource-invariant variables:")
print(consistency_results)
```

# Merge with conflict data

```{r}
# Import conflict data
conflict_data <- read_csv(here("Data", "xSub Conflict Data", "xSub_MELTT1km1dB_COL_adm1_year.csv"))

# Rename the column in conflict data to match the admin1 column in resource data
conflict_data <- conflict_data |> 
  rename(admin1 = NAME_1) |> 
  rename(year = YEAR)

# Print distinct values of admin1 in conflict data
conflict_data |> 
  select(admin1) |> 
  distinct() |> 
  pull()

# Print distinct values of admin1 in resource data
colombia_wide |> 
  select(admin1) |> 
  distinct() |> 
  pull()
```

Let's standardise the admin1 names.

```{r}
# Define the two vectors of names
resource_data_names <- c("Antioquia", "Arauca", "Atlántico", "Bolívar", "Boyacá", 
                         "Córdoba", "Caldas", "Caquetá", "Casanare", "Cauca", "Cesar", 
                         "Chocó", "Cundinamarca", "Guainía", "Guaviare", "Huila", 
                         "La Guajira", "Magdalena", "Meta", "Nariño", "Norte de Santander", 
                         "Putumayo", "Quindío", "Risaralda", "San Andrés y Providencia", 
                         "Santander", "Sucre", "Tolima", "Valle del Cauca", "Vaupés", "Vichada")

conflict_data_names <- c("antioquia", "atlantico", "bogota", "bolivar", "boyaca", 
                         "caldas", "caqueta", "casanare", "cauca", "cesar", "choco", 
                         "cordoba", "cundinamarca", "guainia", "huila", "la guajira", 
                         "norte de santander", "putumayo", "santander", "sucre", 
                         "tolima", "valle del cauca")

# Convert all names to lower case for a fair comparison
resource_data_names <- tolower(resource_data_names)
conflict_data_names <- tolower(conflict_data_names)

# Use set operations to find names in resource data not in conflict data
names_not_in_conflict <- setdiff(resource_data_names, conflict_data_names)

# Output the names not present in the conflict data
print(names_not_in_conflict)

```


Now merge based on year and admin1. First I rename the column in the conflict data to admin1 to match the resource data.


```{r}
# Merge the conflict data with the resource data
merged_data <- left_join(colombia_wide, conflict_data, by = c("admin1", "year"))
```

Let's check how many rows in the merged data have nonNA and non-0 values for annuallocationcapacity_oil

```{r}
print(names(merged_data))
```

